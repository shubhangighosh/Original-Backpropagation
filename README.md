# Original-Backpropagation
Implementation of back-propagation from scratch
Back-propagation was implemented for a neural network with one hidden layer.
Loss function: Squared error with L2-regularisation
Weights were observed to decrease as the regularisation parameter is increased. This observation is expected.
This leads to a restricted search space, and thus is expected to give lower test errors.
Weights for different values of regularisation parameter is reported.

